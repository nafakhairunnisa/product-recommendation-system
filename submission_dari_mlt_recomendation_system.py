# -*- coding: utf-8 -*-
"""Copy of final plz new dataset_bismillah final_Submission dari MLT: Recomendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1snal8AxxbdEZVRSVbqmOBDkA9AbvO1Ok

# Proyek Sistem Rekomendasi: [Amazon Sales Dataset](https://www.kaggle.com/datasets/karkavelrajaj/amazon-sales-dataset)
- **Nama:** Nafa Khairunnisa
- **Email:** nkhairunn2412@gmail.com
- **ID Dicoding:** nafa-khairunnisa

## Import Libraries
"""

!pip uninstall -y numpy scikit-surprise
!pip install numpy==1.24.3 scikit-surprise

!pip install scikit-surprise

# Commented out IPython magic to ensure Python compatibility.
# Library yang sering digunakan
import os
import re
import shutil
from google.colab import drive
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from collections import defaultdict
# %matplotlib inline
import seaborn as sns

# Library untuk data preparation
from sklearn.preprocessing import MinMaxScaler

# Library untuk modeling
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from surprise import Dataset, Reader, SVDpp
from surprise.model_selection import GridSearchCV
from surprise.model_selection import cross_validate
from surprise.model_selection import train_test_split
from surprise import accuracy

# Library untuk text preprocessing
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download semua resource yang relevan
nltk.download('stopwords')

"""## Data Loading"""

# Mount Google Drive
drive.mount('/content/drive')

# Cek apakah file tersedia
!ls /content/drive/MyDrive/kaggle/

!mkdir -p ~/.kaggle
!cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Membuat folder .kaggle
os.makedirs("/root/.kaggle", exist_ok=True)

# Memindahkan kaggle.json dari folder di Google Drive ke folder .kaggle
shutil.move('/content/drive/MyDrive/kaggle/kaggle.json', '/root/.kaggle/kaggle.json')

# Mengubah permission agar hanya bisa dibaca oleh user
os.chmod('/root/.kaggle/kaggle.json', 600)

# Download dataset dari Kaggle
!kaggle datasets download -d karkavelrajaj/amazon-sales-dataset

# Unzip file dataset
!unzip amazon-sales-dataset.zip

# Memuat file csv ke dalam DataFrame
df = pd.read_csv('/content/amazon.csv')

# Menampilkan DataFrame
df.head()

"""## Data Understanding

### Deskripsi Variabel
"""

df.describe()

df.info()

print(df.shape)

"""Insights:

- Dataset memiliki total 1465 sampel.
- Memiliki 16 fitur.
- Semua fitur bertipe data object.
- Nilai unique setiap fitur bervariasi.

### Cek Missing value
"""

df.isna().sum()

"""**Insight**:

Hanya fitur rating_count yang memiliki missing value

### Cek Duplikat Data
"""

df.duplicated().sum()

"""**Insights**:

Dataset tidak memiliki data duplikat.

### Univariate Data Analysis
"""

df['category'].value_counts().head(5)

"""**Insights**:

- Kategori di setiap transaksi bertingkat (memilik hierarki).
- 'Computers&Accessories|Accessories&Peripherals|Cables&Accessories|Cables|USBCables' merupakan category yang paling banyak muncul.
"""

# Plot distribusi 'discount_percentage'
plt.figure(figsize=(10, 4))
df['discount_percentage'].value_counts().head(5).plot(kind='bar') # menampilkan 5 teratas
plt.title(f'Distribusi discount_percentage')
plt.ylabel('Jumlah')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Insights**:

Persentase diskon yang paling banyak muncul yaitu 50%, 60%, 0%, 80%, dan 55%.
"""

# Plot distribusi 'rating'
plt.figure(figsize=(10, 4))
df['rating'].value_counts().head(5).plot(kind='bar') # menampilkan 5 teratas
plt.title(f'Distribusi rating')
plt.ylabel('Jumlah')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**Insights**:

Rating yang diberikan paling banyak berada pada nilai 4.1, 4.3, 4.2, 4.0, dan 3.9. Artinya kualitas produk baik.

## Model Development dengan Content Based Filtering

### Data Preparation
"""

# Menyalin fitur yang akan digunakan
preparation_df = df[['product_name', 'category', 'about_product',
                     'discounted_price', 'actual_price',
                     'discount_percentage', 'rating', 'rating_count']].copy()

preparation_df.info()

preparation_df.head()

# Cek duplikat
preparation_df.duplicated().sum()

# Menghapus data duplikat
preparation_df = preparation_df.drop_duplicates()
print(preparation_df.shape)

# Mengisi nilai NaN dengan 0 kemudian menghapus koma pada rating_count dan mengonversinya ke int
preparation_df['rating_count'] = preparation_df['rating_count'].str.replace(',', '').fillna('0').astype(int)

# Mengecek nilai rating
preparation_df['rating'].unique()

# Mengonversi kolom rating menjadi float, mengganti nilai non-numerik (seperti '|') dengan NaN
preparation_df['rating'] = pd.to_numeric(preparation_df['rating'], errors='coerce')

# Menangani NaN dengan rata-rata rating
preparation_df['rating'] = preparation_df['rating'].fillna(preparation_df['rating'].mean())

# Memeriksa hasil
print(preparation_df['rating'].head())

# Mengganti tanda ₹ menjadi , dan mengonversi tipe data ke float
def clean_price(price_str):
    return float(price_str.replace('₹', '').replace(',', '').strip())

preparation_df['discounted_price'] = preparation_df['discounted_price'].apply(clean_price)
preparation_df['actual_price'] = preparation_df['actual_price'].apply(clean_price)

# Menghilangkan tanda % dan konversi ke tipe float
preparation_df['discount_percentage'] = preparation_df['discount_percentage'].str.replace('%', '').astype(float)

"""Fitur category pada setiap produk terdiri dari beberapa tingkatan (misalnya: Electronics | Computers | Cables). Oleh karena itu, category, product_name, dan about_product digabungkan ke dalam satu kolom teks gabungan (content_text) sebagai representasi deskriptif dari setiap produk. Selain itu, kategori yang paling spesifik (bagian paling kanan setelah simbol "|") juga diekstrak secara terpisah untuk eksplorasi."""

# Memuat model bahasa Inggris spaCy
nlp = spacy.load("en_core_web_sm")

# Fungsi preprocessing dengan spaCy
def preprocess_text(text):
    # Mengubah teks menjadi huruf kecil
    text = str(text).lower()
    # Mengganti karakter selain huruf dengan spasi
    text = re.sub(r'[^a-z\s]', ' ', text)

    # Tokenisasi dan lemmatization dengan spaCy
    doc = nlp(text)

    # Menghapus stop words dan melakukan lemmatization
    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]

    # Menggabungkan token menjadi satu string
    return ' '.join(tokens)

# Mengambil kategori paling spesifik (setelah tanda "|")
preparation_df['category_last'] = preparation_df['category'].apply(lambda x: str(x).split('|')[-1])

# Menggabungkan semua deskripsi produk
preparation_df['content_text'] = preparation_df['product_name'].astype(str) + ' ' + \
                     preparation_df['about_product'].astype(str) + ' ' + \
                     preparation_df['category_last'].astype(str)

# Text Preprocessing
preparation_df['content_clean'] = preparation_df['content_text'].apply(preprocess_text)

# Inisialisasi MinMaxScaler
scaler = MinMaxScaler()

# Melakukan normalisasi pada kolom numerik
numeric_columns = ['discounted_price', 'actual_price', 'rating_count']

preparation_df[numeric_columns] = scaler.fit_transform(preparation_df[numeric_columns])

# Menyalin fitur numerik yang sudah dibersihkan ke dataframe baru
numerical_features = preparation_df[numeric_columns].copy()

# TF-IDF untuk fitur teks
tfidf = TfidfVectorizer(max_features=5000)
tfidf_matrix = tfidf.fit_transform(preparation_df['content_clean'])

"""### Modeling"""

# Gabungkan TF-IDF + numerik (hasil = matriks gabungan)
combined_features = hstack([tfidf_matrix, numerical_features])

# Compute cosine similarity
cosine_sim = cosine_similarity(combined_features)

# Konversi ke DataFrame untuk memudahkan indexing
cosine_sim_df = pd.DataFrame(cosine_sim, index=preparation_df.index, columns=preparation_df.index)

"""### Mendapatkan Rekomendasi"""

def content_based_recommendation(product_index, similarity_matrix, products_df, k=10, threshold=0.9):
    """
    Memberikan rekomendasi produk serupa berdasarkan kemiripan konten produk.

    Parameters:
    ---
    product_index : int
        Indeks produk yang ingin dicari rekomendasinya.
    similarity_matrix : pd.DataFrame
        Matriks cosine similarity antar produk.
    products_df : pd.DataFrame
        Data produk (minimal harus mengandung nama produk dan interest).
    k : int
        Jumlah rekomendasi yang dihasilkan.
    """
    similarity_scores = similarity_matrix.iloc[product_index]
    similarity_scores = similarity_scores.drop(product_index)  # Exclude self
    filtered = similarity_scores[similarity_scores > threshold]  # Filter by threshold
    top_k = filtered.sort_values(ascending=False).head(k)  # Take top-k
    recommended_products = products_df.iloc[top_k.index].copy()
    recommended_products['Similarity_Score'] = top_k.values
    return recommended_products

df.iloc[10]

# Rekomendasi untuk produk ke-10
recommendations = content_based_recommendation(
    product_index=10,
    similarity_matrix=cosine_sim_df,  # <-- gunakan DataFrame di sini
    products_df=df[['product_name', 'category', 'discounted_price']],
    k=10,
    threshold=0.2
)

recommendations

"""### Evaluasi

Sistem rekomendasi dapat dievaluasi menggunakan metrik Precision@K, sebagaimana dijelaskan di situs [evidentlyai](https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems).

Rumus yang digunakan adalah:

```
Precision@K = Jumlah rekomendasi yang relevan/Jumlah rekomendasi yang diberikan
```

Fitur yang dijadikan evaluasi di sini Product_Category_Preference.
"""

# Fungsi Precision@K
def precision_at_k(recommended_indices, relevant_indices, k):
    if k == 0:
        return 0.0
    recommended_at_k = recommended_indices[:k]
    relevant_recommended = [idx for idx in recommended_at_k if idx in relevant_indices]
    return len(relevant_recommended) / k

# Mengambil category_last dari preparation_df
target_category = preparation_df.loc[10, 'category_last']

# Mencari indeks produk lain dengan kategori sama
relevant_indices = set(preparation_df[preparation_df['category_last'] == target_category].index) - {10}

# Mengambil indeks produk yang direkomendasikan
recommended_indices = recommendations.index.tolist()

# Hitung Precision@10
p_at_10 = precision_at_k(recommended_indices, relevant_indices, k=10)
print(f"Precision@10: {p_at_10:.2f}")

"""Berdasarkan hasi metrik evaluasi, precision@K sebesar 100%. Artinya, dari setiap 10 produk yang direkomendasikan oleh sistem, semua produk memang relevan (satu kategori dengan produk yang sedang dievaluasi).

## Model Development dengan Collaborative Filtering

### Data Preparation
"""

# Memilih fitur relevan untuk user-based collaborative filtering
cf_preparation = df[['user_id', 'product_id', 'rating']].copy()

# Menampilkan beberapa baris pertama
cf_preparation.head()

# Cek duplikat
cf_preparation.duplicated().sum()

# Menghapus data duplikat
cf_preparation = cf_preparation.drop_duplicates()
print(cf_preparation.shape)

# Konversi tipe data rating dari object ke float
# Mengonversi kolom rating menjadi float, mengganti nilai non-numerik (seperti '|') dengan NaN
cf_preparation['rating'] = pd.to_numeric(cf_preparation['rating'], errors='coerce')

# Menangani NaN dengan rata-rata rating
cf_preparation['rating'] = cf_preparation['rating'].fillna(cf_preparation['rating'].mean())

# Memeriksa hasil
print(cf_preparation['rating'].head())

# Encoding user dan product
user_ids = cf_preparation['user_id'].unique().tolist()
product_ids = cf_preparation['product_id'].unique().tolist()

user_to_encoded = {x: i for i, x in enumerate(user_ids)}
product_to_encoded = {x: i for i, x in enumerate(product_ids)}

cf_preparation['user'] = cf_preparation['user_id'].map(user_to_encoded)
cf_preparation['product'] = cf_preparation['product_id'].map(product_to_encoded)

# Info dasar
num_users = len(user_to_encoded)
num_products = len(product_to_encoded)
min_rating = cf_preparation['rating'].min()
max_rating = cf_preparation['rating'].max()
print(f"User: {num_users}, Product: {num_products}, Min Rating: {min_rating}, Max Rating: {max_rating}")

# Mengacak data
cf_preparation = cf_preparation.sample(frac=1, random_state=42)

# Dataset untuk Surprise
reader = Reader(rating_scale=(min_rating, max_rating))
data = Dataset.load_from_df(cf_preparation[['user', 'product', 'rating']], reader)

# Split train-test
train_set, test_set = train_test_split(data, test_size=0.2)

"""### Modeling"""

# Grid Search
param_grid = {
    'n_factors': [20, 50, 100, 150, 200],
    'lr_all': [0.001, 0.002, 0.005, 0.01, 0.02],
    'reg_all': [0.005, 0.01, 0.02, 0.05, 0.1]
}
gs = GridSearchCV(SVDpp, param_grid, measures=['rmse'], cv=3, n_jobs=-1, joblib_verbose=1)
gs.fit(data)

print("Best RMSE score:", gs.best_score['rmse'])
print("Best parameters:", gs.best_params['rmse'])

# Evaluasi Cross-Validation dengan best model
best_model = SVDpp(**gs.best_params['rmse'])
cross_validate(best_model, data, measures=['RMSE'], cv=3, verbose=True)

# Fit ulang di train_set
best_model.fit(train_set)

# Evaluasi di test_set
predictions = best_model.test(test_set)

"""### Mendapatkan Top-N Rekomendasi"""

# Membuat reverse mapping (angka -> user_id dan product_id)
encoded_to_user = {i: x for x, i in user_to_encoded.items()}
encoded_to_product = {i: x for x, i in product_to_encoded.items()}

# Mengambil user_id berdasarkan indeks ke-11 dari daftar unik user
user_id = df['user_id'].unique()[10]  # indeks ke-11

# Semua product_id
all_product_ids = df['product_id'].unique()

# Produk yang sudah dinilai user
rated_products = df[df['user_id'] == user_id]['product_id'].tolist()

# Produk yang belum dinilai user (candidate for recommendation)
not_rated = [pid for pid in all_product_ids if pid not in rated_products]

# Prediksi rating untuk setiap produk yang belum dirating
predictions = [best_model.predict(user_id, pid) for pid in not_rated]

# Urutkan berdasarkan rating tertinggi
predictions.sort(key=lambda x: x.est, reverse=True)

# Ambil 10 produk teratas
top_10_predictions = predictions[:10]
recommended_product_ids = [pred.iid for pred in top_10_predictions]

# Tampilkan rekomendasi
print(f'Rekomendasi untuk User {user_id}')
print('=' * 30)

# Produk yang pernah dinilai user
print('Produk dengan rating tinggi dari user:')
print('-' * 30)
top_rated = df[(df['user_id'] == user_id)].sort_values(by='rating', ascending=False).head(5)
for row in top_rated.itertuples():
    print(f"{row.product_name} (Rating: {row.rating})")

# Produk yang direkomendasikan
print('-' * 30)
print('Top 10 Produk Rekomendasi:')
print('-' * 30)
recommended_df = df[df['product_id'].isin(recommended_product_ids)][['product_id', 'product_name', 'category']].drop_duplicates()
for row in recommended_df.itertuples():
    print(f"{row.product_name} - {row.category}")

"""### Evaluasi"""

# Evaluasi model
# Membuat prediksi pada test set
predictions = best_model.test(test_set)

# Evaluasi dengan RMSE
rmse = accuracy.rmse(predictions)
print(f"Test RMSE dengan best parameter: {rmse:.4f}")

# Evaluasi dengan MAE
mae = accuracy.mae(predictions)
print(f"MAE dengan best parameter: {mae:.4f}")

"""Hasil evaluasi model rekomendasi menunjukkan performa yang sangat baik:
- RMSE (Root Mean Squared Error): 0.2670
- MAE (Mean Absolute Error): 0.2013

Semakin rendah nilai RMSE dan MAE, semakin akurat model dalam memprediksi rating yang mendekati nilai sebenarnya.
"""